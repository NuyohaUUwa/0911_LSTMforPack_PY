import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping
from datetime import datetime
from common import load_json_data, prepare_dataset, build_model


# ========== è®­ç»ƒå‡½æ•° ==========
def infinite_training(X, y, model_path="models/model_checkpoint.keras", max_epochs=None, step_name="æœªçŸ¥æ­¥éª¤", training_type="full"):
    """
    æ”¯æŒæ— é™è®­ç»ƒæˆ–å›ºå®šè½®æ•°è®­ç»ƒï¼Œè‡ªåŠ¨ä¿å­˜æ¨¡å‹ï¼Œå¹¶ç»˜åˆ¶ loss æ›²çº¿ã€‚
    
    å‚æ•°ï¼š
    - max_epochs=Noneï¼šæ— é™è®­ç»ƒç›´åˆ°æ‰‹åŠ¨ä¸­æ–­
    - max_epochs=intï¼šå›ºå®šè½®æ•°è®­ç»ƒ
    """
    model = build_model((X.shape[1], X.shape[2]))

    # æ ¹æ®è®­ç»ƒç±»å‹å’Œæ•°æ®é‡è®¾ç½®ä¸åŒçš„æ—©åœç­–ç•¥
    data_size = len(X)
    if training_type == "validation":
        # éªŒè¯æ¨¡å¼ï¼šæ›´æ¿€è¿›çš„æ—©åœ
        patience = min(20, max(10, data_size // 2))
        min_delta = 0.001
    else:
        # å®Œæ•´è®­ç»ƒæ¨¡å¼ï¼šæ›´å®½æ¾çš„æ—©åœ
        patience = min(50, max(20, data_size))
        min_delta = 0.0005
    
    early_stop = EarlyStopping(
        monitor='loss', 
        patience=patience,
        min_delta=min_delta,
        restore_best_weights=True,
        verbose=1
    )
    
    print(f"ğŸ”§ æ—©åœè®¾ç½®: patience={patience}, min_delta={min_delta}")

    losses = []
    epoch = 0

    if max_epochs is None:
        # æ— é™è®­ç»ƒï¼ˆå»ºè®®Ctrl+Cæ‰“æ–­ï¼‰
        try:
            while True:
                history = model.fit(X, y, epochs=1, verbose=0, callbacks=[early_stop])
                epoch += 1
                loss = history.history['loss'][0]
                losses.append(loss)

                if epoch % 10 == 0:
                    print(f"ğŸ“š ç¬¬ {epoch} æ¬¡è®­ç»ƒï¼ŒæŸå¤±: {loss:.6f}")
                    model.save(model_path)
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ æ‰‹åŠ¨ç»ˆæ­¢è®­ç»ƒï¼ˆç¬¬ {epoch} æ¬¡ï¼‰")
    else:
        # å›ºå®šæ¬¡æ•°è®­ç»ƒ
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆå…± {max_epochs} æ¬¡ï¼‰...")
        history = model.fit(X, y, epochs=max_epochs, callbacks=[early_stop], verbose=1)
        losses = history.history['loss']
        print("âœ… å›ºå®šè½®è®­ç»ƒå®Œæˆ")

    # æœ€åä¿å­˜ä¸€æ¬¡æ¨¡å‹
    model.save(model_path)
    print(f"ğŸ“¦ æ¨¡å‹å·²ä¿å­˜è‡³ {model_path}")
    
    final_loss = model.evaluate(X, y, verbose=0)
    print(f"ğŸ“‰ æœ€ç»ˆæŸå¤±: {final_loss:.6f}")

    # ========== ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿ ========== #
    plt.figure(figsize=(10, 4))
    plt.plot(losses, label="Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f"{step_name} è®­ç»ƒè¿‡ç¨‹ Loss æ›²çº¿")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    
    # ä¿å­˜è®­ç»ƒæŸå¤±æ›²çº¿
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    filename = f"All_plotpics/TrainingLoss_{step_name}_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜: {filename}")
    
    plt.show()
    
    return model


# ========== å…¨æµç¨‹è®­ç»ƒå‡½æ•° ==========
def run_full_painting_workflow(df_train, train_epochs, step_names_to_train=None):
    """
    å…¨æµç¨‹è®­ç»ƒæ¨¡å¼ï¼šè®­ç»ƒæŒ‡å®šçš„æ­¥éª¤
    
    å‚æ•°:
    - df_train: è®­ç»ƒæ•°æ®
    - train_epochs: è®­ç»ƒè½®æ¬¡
    - step_names_to_train: è¦è®­ç»ƒçš„æ­¥éª¤åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è¯†åˆ«æ¶‚èƒ¶æ­¥éª¤
    """
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # ç¡®å®šè¦è®­ç»ƒçš„æ­¥éª¤
    if step_names_to_train is None:
        # è‡ªåŠ¨è¯†åˆ«æ‰€æœ‰æ¶‚èƒ¶æ­¥éª¤ï¼ˆå‘åå…¼å®¹ï¼‰
        all_steps = df_train['StepName'].unique()
        steps_to_train = [step for step in all_steps if 'æ¶‚èƒ¶' in step or 'ä¸‹è½®æ¶‚èƒ¶' in step]
        print(f"ğŸ¯ è‡ªåŠ¨è¯†åˆ«åˆ°æ¶‚èƒ¶æ­¥éª¤: {steps_to_train}")
    else:
        # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ­¥éª¤åˆ—è¡¨
        steps_to_train = step_names_to_train
        print(f"ğŸ¯ ç”¨æˆ·æŒ‡å®šè¦è®­ç»ƒçš„æ­¥éª¤: {steps_to_train}")
    
    # æ£€æŸ¥æ•°æ®ä¸­å®é™…å­˜åœ¨çš„æ­¥éª¤
    available_steps = df_train['StepName'].unique()
    valid_steps = [step for step in steps_to_train if step in available_steps]
    missing_steps = [step for step in steps_to_train if step not in available_steps]
    
    if missing_steps:
        print(f"âš ï¸ ä»¥ä¸‹æ­¥éª¤åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_steps}")
    
    if not valid_steps:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ­¥éª¤è¿›è¡Œè®­ç»ƒï¼")
        return
    
    print(f"ğŸ“Š æœ‰æ•ˆè®­ç»ƒæ­¥éª¤æ•°: {len(valid_steps)}")
    print(f"ğŸ“‹ å¯ç”¨æ­¥éª¤åˆ—è¡¨: {valid_steps}")
    
    for step_name in valid_steps:
        print(f"\n\n================= ğŸš€ å…¨æµç¨‹è®­ç»ƒï¼š{step_name} =================")
        
        try:
            # ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒ
            X_train, y_train, scaler, df_filtered = prepare_dataset(df_train, step_name=step_name)
            
            print(f"ğŸ“Š è®­ç»ƒæ•°æ®ä¸­ {step_name} æ­¥éª¤æ€»æ•°é‡: {len(df_filtered)}")
            print(f"ğŸ“š ä½¿ç”¨å…¨éƒ¨ {len(df_filtered)} ä¸ªæ•°æ®ç‚¹è¿›è¡Œå®Œæ•´è®­ç»ƒ...")
            
            model_file = f"models/model_{step_name}.keras"
            
            # å®Œæ•´è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
            print(f"ğŸš€ å¼€å§‹å®Œæ•´è®­ç»ƒï¼ˆå…± {train_epochs} è½®ï¼‰...")
            model = infinite_training(X_train, y_train, model_path=model_file, max_epochs=train_epochs, step_name=step_name, training_type="full")
            
            print(f"âœ… {step_name} é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜è‡³ {model_file}")
            print(f"ğŸ“ˆ è®­ç»ƒæ•°æ®é‡: {len(df_filtered)} æ¡")
            print(f"ğŸ¯ æ¨¡å‹å¯ç”¨äºåç»­é¢„æµ‹ä»»åŠ¡")

        except Exception as e:
            print(f"âŒ æ­¥éª¤ {step_name} è®­ç»ƒå¤±è´¥ï¼š{e}")


# ========== æ•°æ®åº“ç›¸å…³å‡½æ•° ==========
def build_pgsql_conditions(step_names_to_train):
    """
    æ ¹æ®æ­¥éª¤åç§°åˆ—è¡¨ç”Ÿæˆ PostgreSQL WHERE æ¡ä»¶
    
    å‚æ•°:
    - step_names_to_train: æ­¥éª¤åç§°åˆ—è¡¨
    
    è¿”å›:
    - SQL WHERE æ¡ä»¶å­—ç¬¦ä¸²ï¼Œå¦‚æœåˆ—è¡¨ä¸ºç©ºåˆ™è¿”å› None
    """
    if step_names_to_train:
        step_conditions = "', '".join(step_names_to_train)
        # ä½¿ç”¨åŒå¼•å·åŒ…è£¹å­—æ®µåä»¥ä¿ç•™å¤§å°å†™
        return f'"StepName" IN (\'{step_conditions}\')'
    else:
        return None


def load_training_data(data_source, json_file=None, pgsql_query=None, pgsql_table=None, pgsql_conditions=None):
    """
    ç»Ÿä¸€çš„æ•°æ®åŠ è½½æ¥å£ï¼Œæ ¹æ®æ•°æ®æºç±»å‹åŠ è½½æ•°æ®
    
    å‚æ•°:
    - data_source: æ•°æ®æºç±»å‹ ("json" æˆ– "pgsql")
    - json_file: JSON æ–‡ä»¶è·¯å¾„ï¼ˆå½“ data_source="json" æ—¶ä½¿ç”¨ï¼‰
    - pgsql_query: è‡ªå®šä¹‰ SQL æŸ¥è¯¢è¯­å¥ï¼ˆå½“ data_source="pgsql" æ—¶ä½¿ç”¨ï¼‰
    - pgsql_table: è¡¨åï¼ˆå½“ data_source="pgsql" æ—¶ä½¿ç”¨ï¼‰
    - pgsql_conditions: WHERE æ¡ä»¶ï¼ˆå½“ data_source="pgsql" æ—¶ä½¿ç”¨ï¼‰
    
    è¿”å›:
    - pandas DataFrame
    """
    print("ğŸ“‚ æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    
    if data_source == "json":
        from common import load_json_data
        if json_file is None:
            raise ValueError("ä½¿ç”¨ JSON æ•°æ®æºæ—¶å¿…é¡»æä¾› json_file å‚æ•°")
        df = load_json_data(json_file)
        
    elif data_source == "pgsql":
        from common import load_pgsql_data
        df = load_pgsql_data(query=pgsql_query, table_name=pgsql_table, conditions=pgsql_conditions)
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {data_source}ï¼Œè¯·ä½¿ç”¨ 'json' æˆ– 'pgsql'")
    
    print("âœ… æ•°æ®åŠ è½½å®Œæˆ")
    
    # éªŒè¯æ•°æ®åˆ—
    from common import validate_data_columns
    validate_data_columns(df)
    
    # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
    print(f"ğŸ“Š æ•°æ®æ€»è¡Œæ•°: {len(df)}")
    print(f"ğŸ“‹ åŒ…å«çš„ StepName: {df['StepName'].unique().tolist()}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {df['Timestamp'].min()} ~ {df['Timestamp'].max()}")
    
    return df


def print_training_config(train_epochs, data_source, step_names_to_train, 
                         json_file=None, pgsql_query=None, pgsql_table=None, pgsql_conditions=None):
    """
    æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯
    
    å‚æ•°:
    - train_epochs: è®­ç»ƒè½®æ¬¡
    - data_source: æ•°æ®æºç±»å‹
    - step_names_to_train: è¦è®­ç»ƒçš„æ­¥éª¤åˆ—è¡¨
    - json_file: JSON æ–‡ä»¶è·¯å¾„
    - pgsql_query: SQL æŸ¥è¯¢è¯­å¥
    - pgsql_table: è¡¨å
    - pgsql_conditions: WHERE æ¡ä»¶
    """
    print(f"\nğŸš€ å¼€å§‹å…¨æµç¨‹è®­ç»ƒæ¨¡å¼...")
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   - è®­ç»ƒè½®æ¬¡: {train_epochs}")
    print(f"   - æ•°æ®æºç±»å‹: {data_source}")
    print(f"   - è¦è®­ç»ƒçš„æ­¥éª¤: {step_names_to_train}")
    
    if data_source == "json":
        print(f"   - æ•°æ®æ–‡ä»¶: {json_file}")
        
    elif data_source == "pgsql":
        if pgsql_query:
            print(f"   - SQLæŸ¥è¯¢: {pgsql_query}")
        else:
            print(f"   - æ•°æ®è¡¨: {pgsql_table}")
            if pgsql_conditions:
                print(f"   - æŸ¥è¯¢æ¡ä»¶: {pgsql_conditions}")
    
    print(f"\nğŸ”§ æ•°æ®å¤„ç†æµç¨‹:")
    print(f"   1. æ ¹æ® StepName åˆ†ç±»")
    print(f"   2. æŒ‰ Timestamp æ’åº")
    print(f"   3. ä½¿ç”¨ Duration å€¼è®­ç»ƒ LSTM æ¨¡å‹")


# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    # ========== é…ç½®å‚æ•° ==========
    train_epochs = None  # è®­ç»ƒè½®æ¬¡ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
    
    # ğŸ‘‰ æ•°æ®æºé…ç½®
    data_source = "pgsql"  # æ•°æ®æºç±»å‹: "json" æˆ– "pgsql"
    
    # ğŸ‘‰ è¦è®­ç»ƒçš„ StepName é…ç½®ï¼ˆæ‚¨å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹è¿™ä¸ªåˆ—è¡¨ï¼‰
    step_names_to_train = [
        "æ‹§ç´§1",    # Tighten 1
        "æ‹§ç´§2",    # Tighten 2  
        "æ‹§ç´§3",    # Tighten 3
        "å¤¹ç´§",     # Clamp
        "æ¾å¼€",     # Loosen
        "å–é’‰1",    # Remove Nail 1
        "å–é’‰2"     # Remove Nail 2
    ]
    
    # JSON æ•°æ®æºé…ç½®
    train_data_file = "data/OnlyPainting_Paint_Data3_Train.json"  # è®­ç»ƒæ•°æ®æ–‡ä»¶
    
    # PostgreSQL æ•°æ®æºé…ç½®ï¼ˆå½“ data_source = "pgsql" æ—¶ä½¿ç”¨ï¼‰
    pgsql_query = None  # è‡ªå®šä¹‰SQLæŸ¥è¯¢è¯­å¥ï¼Œä¾‹å¦‚: "SELECT StepName, Duration, Timestamp FROM Beats_of_M8_liangainingjin WHERE StepName LIKE '%æ¶‚èƒ¶%'"
    pgsql_table = '"Beats_of_M8_liangainingjin"'  # è¡¨å
    
    # ========== æ•°æ®åŠ è½½å’Œé…ç½® ==========
    # ç”Ÿæˆ PostgreSQL WHERE æ¡ä»¶
    pgsql_conditions = build_pgsql_conditions(step_names_to_train)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    df_train = load_training_data(
        data_source=data_source,
        json_file=train_data_file,
        pgsql_query=pgsql_query,
        pgsql_table=pgsql_table,
        pgsql_conditions=pgsql_conditions
    )
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print_training_config(
        train_epochs=train_epochs,
        data_source=data_source,
        step_names_to_train=step_names_to_train,
        json_file=train_data_file,
        pgsql_query=pgsql_query,
        pgsql_table=pgsql_table,
        pgsql_conditions=pgsql_conditions
    )
    
    run_full_painting_workflow(df_train, train_epochs, step_names_to_train)
    print(f"\nğŸ‰ å…¨æµç¨‹è®­ç»ƒå®Œæˆï¼æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜ã€‚")

