import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from datetime import datetime
from tensorflow.keras.models import load_model
from common import load_json_data, prepare_dataset


# ========== é¢„æµ‹å‡½æ•° ==========
def predict_future(df, model_path, scaler, step_name="zq", seq_len=10, predict_count=10):
    """ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œé¢„æµ‹"""
    try:
        model = load_model(model_path)
    except Exception as e:
        raise ValueError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    
    # è·å–ç›®æ ‡æ­¥éª¤çš„æ•°æ®
    df_filtered = df[df["StepName"] == step_name].copy()
    
    # æ¸…ç† NaN å€¼å¹¶ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
    if df_filtered.empty:
        raise ValueError(f"âŒ '{step_name}' æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å½•")
        
    # ç§»é™¤ Duration ä¸º NaN çš„è®°å½•
    df_filtered = df_filtered.dropna(subset=["Duration"])
    
    # é‡ç½®ç´¢å¼•ä»¥ä¾¿åç»­ä½¿ç”¨è´Ÿç´¢å¼• [-seq_len:]
    df_filtered.reset_index(drop=True, inplace=True)
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹
    if len(df_filtered) < seq_len:
        raise ValueError(f"âŒ '{step_name}' çš„æœ‰æ•ˆæ•°æ®ä¸è¶³ {seq_len} æ¡ï¼Œä»…æœ‰ {len(df_filtered)} æ¡")
    
    # è½¬æ¢æ•°æ®ç±»å‹å¹¶æ£€æŸ¥å¼‚å¸¸å€¼
    durations = df_filtered["Duration"].astype(float).values.reshape(-1, 1)
    
    # æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥
    if np.any(np.isnan(durations)):
        # è®°å½•æœ‰é—®é¢˜çš„è¡Œç´¢å¼•
        nan_indices = np.where(np.isnan(durations.flatten()))[0]
        problem_timestamps = df_filtered.iloc[nan_indices]["Timestamp"].tolist()
        raise ValueError(
            f"âŒ '{step_name}' æ•°æ®å¼‚å¸¸: {len(nan_indices)} æ¡è®°å½• Duration å€¼æ— æ•ˆ\n"
            f"é—®é¢˜è®°å½•æ—¶é—´: {problem_timestamps}"
        )
    
    # ç¼©æ”¾æ•°æ®
    durations_scaled = scaler.transform(durations)
    
    # è·å–æœ€æ–°çš„åºåˆ—ä½œä¸ºæ¨¡å‹è¾“å…¥
    current_seq = durations_scaled[-seq_len:]
    
    # åˆ›å»ºé¢„æµ‹å­˜å‚¨åˆ—è¡¨
    predictions = []

    for i in range(predict_count):
        input_seq = current_seq.reshape(1, seq_len, 1)
        pred = model.predict(input_seq, verbose=0)[0][0]
        if np.isnan(pred) or np.isinf(pred):
            print(f"ğŸš« é¢„æµ‹å¤±è´¥: ç¬¬ {i + 1} æ¬¡é¢„æµ‹ç»“æœæ— æ•ˆ")
            break
        predictions.append(pred)
        current_seq = np.append(current_seq[1:], [[pred]], axis=0)

    if len(predictions) == 0:
        print("âš ï¸ æ— æœ‰æ•ˆé¢„æµ‹ç»“æœ")
        return [], df_filtered

    predictions_array = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
    return predictions_array, df_filtered


def predict_future_with_model(model, df, scaler, step_name="zq", seq_len=10, predict_count=10):
    """ä½¿ç”¨æ¨¡å‹å¯¹è±¡è¿›è¡Œé¢„æµ‹ï¼Œé¿å…æ–‡ä»¶åŠ è½½é—®é¢˜"""
    # è·å–ç›®æ ‡æ­¥éª¤çš„æ•°æ®
    df_filtered = df[df["StepName"] == step_name].copy()
    
    # æ¸…ç† NaN å€¼å¹¶ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
    if df_filtered.empty:
        raise ValueError(f"âŒ '{step_name}' æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å½•")
        
    # ç§»é™¤ Duration ä¸º NaN çš„è®°å½•
    df_filtered = df_filtered.dropna(subset=["Duration"])
    
    # é‡ç½®ç´¢å¼•ä»¥ä¾¿åç»­ä½¿ç”¨è´Ÿç´¢å¼• [-seq_len:]
    df_filtered.reset_index(drop=True, inplace=True)
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹
    if len(df_filtered) < seq_len:
        raise ValueError(f"âŒ '{step_name}' çš„æœ‰æ•ˆæ•°æ®ä¸è¶³ {seq_len} æ¡ï¼Œä»…æœ‰ {len(df_filtered)} æ¡")
    
    # è½¬æ¢æ•°æ®ç±»å‹å¹¶æ£€æŸ¥å¼‚å¸¸å€¼
    durations = df_filtered["Duration"].astype(float).values.reshape(-1, 1)
    
    # æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥
    if np.any(np.isnan(durations)):
        # è®°å½•æœ‰é—®é¢˜çš„è¡Œç´¢å¼•
        nan_indices = np.where(np.isnan(durations.flatten()))[0]
        problem_timestamps = df_filtered.iloc[nan_indices]["Timestamp"].tolist()
        raise ValueError(
            f"âŒ '{step_name}' æ•°æ®å¼‚å¸¸: {len(nan_indices)} æ¡è®°å½• Duration å€¼æ— æ•ˆ\n"
            f"é—®é¢˜è®°å½•æ—¶é—´: {problem_timestamps}"
        )
    
    # ç¼©æ”¾æ•°æ®
    durations_scaled = scaler.transform(durations)
    
    # è·å–æœ€æ–°çš„åºåˆ—ä½œä¸ºæ¨¡å‹è¾“å…¥
    current_seq = durations_scaled[-seq_len:]
    
    # åˆ›å»ºé¢„æµ‹å­˜å‚¨åˆ—è¡¨
    predictions = []

    for i in range(predict_count):
        input_seq = current_seq.reshape(1, seq_len, 1)
        pred = model.predict(input_seq, verbose=0)[0][0]
        if np.isnan(pred) or np.isinf(pred):
            print(f"ğŸš« é¢„æµ‹å¤±è´¥: ç¬¬ {i + 1} æ¬¡é¢„æµ‹ç»“æœæ— æ•ˆ")
            break
        predictions.append(pred)
        current_seq = np.append(current_seq[1:], [[pred]], axis=0)

    if len(predictions) == 0:
        print("âš ï¸ æ— æœ‰æ•ˆé¢„æµ‹ç»“æœ")
        return [], df_filtered

    predictions_array = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
    return predictions_array, df_filtered


# ========== å¯è§†åŒ–å‡½æ•° ==========
def plot_predictions(df_filtered, pred_durations):
    """ç»˜åˆ¶é¢„æµ‹ç»“æœå›¾è¡¨"""
    plt.figure(figsize=(16, 6))
    plt.plot(df_filtered["Duration"].values, label="å†å²æ•°æ®")
    plt.plot(range(len(df_filtered), len(df_filtered) + len(pred_durations)), pred_durations, "o-", color="orange",
             label="é¢„æµ‹")

    for i, v in enumerate(df_filtered["Duration"].values):
        plt.text(i, v + 0.03, f"{v:.2f}", ha="center", fontsize=8)
    for i, v in enumerate(pred_durations):
        plt.text(len(df_filtered) + i, v + 0.03, f"{v:.2f}", ha="center", fontsize=8, color="orange")

    labels = df_filtered["StepName"].tolist() + [f"é¢„æµ‹{i + 1}" for i in range(len(pred_durations))]
    plt.xticks(ticks=range(len(labels)), labels=labels, rotation=45)
    plt.xlabel("æ­¥éª¤")
    plt.ylabel("Durationï¼ˆç§’ï¼‰")
    plt.title(f"{df_filtered.iloc[0]['StepName']} æ­¥éª¤è€—æ—¶é¢„æµ‹")
    plt.legend()
    plt.tight_layout()
    
    # ä¿å­˜é¢„æµ‹ç»“æœå›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    step_name = df_filtered.iloc[0]['StepName']
    filename = f"All_plotpics/Prediction_{step_name}_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š é¢„æµ‹ç»“æœå›¾è¡¨å·²ä¿å­˜: {filename}")
    
    plt.show()


def plot_predictions_with_validation(df_filtered, pred_durations, true_values, step_name):
    """ç»˜åˆ¶åŒ…å«éªŒè¯å¯¹æ¯”çš„é¢„æµ‹å›¾è¡¨"""
    plt.figure(figsize=(18, 8))
    
    # è®­ç»ƒæ•°æ®ï¼ˆå†å²æ•°æ®ï¼‰
    train_data = df_filtered["Duration"].values
    plt.plot(train_data, "b-", label="è®­ç»ƒæ•°æ®", linewidth=2, marker='o', markersize=4)
    
    # é¢„æµ‹å€¼
    pred_start_idx = len(train_data)
    pred_indices = range(pred_start_idx, pred_start_idx + len(pred_durations))
    plt.plot(pred_indices, pred_durations, "r-", label="é¢„æµ‹å€¼", linewidth=2, marker='s', markersize=6)
    
    # çœŸå®å€¼ï¼ˆç”¨äºéªŒè¯ï¼‰
    true_indices = range(pred_start_idx, pred_start_idx + len(true_values))
    plt.plot(true_indices, true_values, "g-", label="çœŸå®å€¼", linewidth=2, marker='^', markersize=6)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(train_data):
        plt.text(i, v + 0.05, f"{v:.2f}", ha="center", fontsize=8, color="blue")
    
    for i, v in enumerate(pred_durations):
        plt.text(pred_start_idx + i, v + 0.05, f"{v:.2f}", ha="center", fontsize=8, color="red")
    
    for i, v in enumerate(true_values):
        plt.text(pred_start_idx + i, v - 0.1, f"{v:.2f}", ha="center", fontsize=8, color="green")
    
    # æ·»åŠ è¯¯å·®çº¿
    for i, (pred, true) in enumerate(zip(pred_durations, true_values)):
        plt.plot([pred_start_idx + i, pred_start_idx + i], [pred, true], 
                "k--", alpha=0.5, linewidth=1)
        error = abs(pred - true)
        plt.text(pred_start_idx + i, (pred + true) / 2, f"è¯¯å·®:{error:.3f}", 
                ha="center", fontsize=7, color="purple")
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆåœ¨è®¾ç½®å›¾è¡¨å±æ€§ä¹‹å‰ï¼‰
    mae = np.mean(np.abs(pred_durations - true_values))
    mse = np.mean((pred_durations - true_values) ** 2)
    rmse = np.sqrt(mse)
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    plt.xlabel("æ•°æ®ç‚¹ç´¢å¼•")
    plt.ylabel("Durationï¼ˆç§’ï¼‰")
    plt.title(f"{step_name} æ­¥éª¤è€—æ—¶é¢„æµ‹éªŒè¯å¯¹æ¯”\nè®­ç»ƒæ•°æ®: {len(train_data)}ä¸ªç‚¹, é¢„æµ‹: {len(pred_durations)}ä¸ªç‚¹")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ åˆ†ç•Œçº¿
    plt.axvline(x=pred_start_idx-0.5, color='gray', linestyle=':', alpha=0.7, label="è®­ç»ƒ/é¢„æµ‹åˆ†ç•Œçº¿")
    
    # åœ¨å›¾è¡¨ä¸Šæ·»åŠ é¢„æµ‹æ€§èƒ½è¯„ä¼°æŒ‡æ ‡
    metrics_text = f"é¢„æµ‹æ€§èƒ½è¯„ä¼°:\nMAE: {mae:.4f}ç§’\nMSE: {mse:.4f}\nRMSE: {rmse:.4f}ç§’"
    plt.text(0.02, 0.98, metrics_text, 
             transform=plt.gca().transAxes,
             fontsize=10,
             verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    
    # ä¿å­˜éªŒè¯å¯¹æ¯”å›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    filename = f"All_plotpics/Validation_{step_name}_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š éªŒè¯å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {filename}")
    
    plt.show()
    
    # åŒæ—¶åœ¨æ§åˆ¶å°æ‰“å°è¯„ä¼°æŒ‡æ ‡
    print(f"\nğŸ“Š é¢„æµ‹æ€§èƒ½è¯„ä¼°:")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.4f} ç§’")
    print(f"   å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
    print(f"   å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.4f} ç§’")


# ========== æ•°æ®åº“ç›¸å…³å‡½æ•° ==========
def build_pgsql_conditions(step_names_to_predict):
    """
    æ ¹æ®æ­¥éª¤åç§°åˆ—è¡¨ç”Ÿæˆ PostgreSQL WHERE æ¡ä»¶
    
    å‚æ•°:
    - step_names_to_predict: æ­¥éª¤åç§°åˆ—è¡¨
    
    è¿”å›:
    - SQL WHERE æ¡ä»¶å­—ç¬¦ä¸²ï¼Œå¦‚æœåˆ—è¡¨ä¸ºç©ºåˆ™è¿”å› None
    """
    if step_names_to_predict:
        step_conditions = "', '".join(step_names_to_predict)
        # ä½¿ç”¨åŒå¼•å·åŒ…è£¹å­—æ®µåä»¥ä¿ç•™å¤§å°å†™
        return f'"StepName" IN (\'{step_conditions}\')'
    else:
        return None


def load_prediction_data(data_source, json_file=None, pgsql_query=None, pgsql_table=None, pgsql_conditions=None):
    """
    ç»Ÿä¸€çš„æ•°æ®åŠ è½½æ¥å£ï¼Œæ ¹æ®æ•°æ®æºç±»å‹åŠ è½½æ•°æ®
    
    å‚æ•°:
    - data_source: æ•°æ®æºç±»å‹ ("json" æˆ– "pgsql")
    - json_file: JSON æ–‡ä»¶è·¯å¾„ï¼ˆå½“ data_source="json" æ—¶ä½¿ç”¨ï¼‰
    - pgsql_query: è‡ªå®šä¹‰ SQL æŸ¥è¯¢è¯­å¥ï¼ˆå½“ data_source="pgsql" æ—¶ä½¿ç”¨ï¼‰
    - pgsql_table: è¡¨åï¼ˆå½“ data_source="pgsql" æ—¶ä½¿ç”¨ï¼‰
    - pgsql_conditions: WHERE æ¡ä»¶ï¼ˆå½“ data_source="pgsql" æ—¶ä½¿ç”¨ï¼‰
    
    è¿”å›:
    - pandas DataFrame
    """
    print("ğŸ“‚ æ­£åœ¨åŠ è½½é¢„æµ‹æ•°æ®...")
    
    if data_source == "json":
        from common import load_json_data
        if json_file is None:
            raise ValueError("ä½¿ç”¨ JSON æ•°æ®æºæ—¶å¿…é¡»æä¾› json_file å‚æ•°")
        df = load_json_data(json_file)
        
    elif data_source == "pgsql":
        from common import load_pgsql_data
        df = load_pgsql_data(query=pgsql_query, table_name=pgsql_table, conditions=pgsql_conditions)
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {data_source}ï¼Œè¯·ä½¿ç”¨ 'json' æˆ– 'pgsql'")
    
    print("âœ… æ•°æ®åŠ è½½å®Œæˆ")
    
    # éªŒè¯æ•°æ®åˆ—
    from common import validate_data_columns
    validate_data_columns(df)
    
    # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
    print(f"ğŸ“Š æ•°æ®æ€»è¡Œæ•°: {len(df)}")
    print(f"ğŸ“‹ åŒ…å«çš„ StepName: {df['StepName'].unique().tolist()}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {df['Timestamp'].min()} ~ {df['Timestamp'].max()}")
    
    return df


def print_prediction_config(num_of_prediction, train_data_count, data_source, step_names_to_predict,
                           json_file=None, pgsql_query=None, pgsql_table=None, pgsql_conditions=None):
    """
    æ‰“å°é¢„æµ‹é…ç½®ä¿¡æ¯
    
    å‚æ•°:
    - num_of_prediction: é¢„æµ‹æ•°é‡
    - train_data_count: è®­ç»ƒæ•°æ®æ•°é‡
    - data_source: æ•°æ®æºç±»å‹
    - step_names_to_predict: è¦é¢„æµ‹çš„æ­¥éª¤åˆ—è¡¨
    - json_file: JSON æ–‡ä»¶è·¯å¾„
    - pgsql_query: SQL æŸ¥è¯¢è¯­å¥
    - pgsql_table: è¡¨å
    - pgsql_conditions: WHERE æ¡ä»¶
    """
    print(f"\nğŸ”§ å¼€å§‹å…¨æµç¨‹é¢„æµ‹éªŒè¯æ¨¡å¼...")
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   - é¢„æµ‹æ•°é‡: {num_of_prediction}")
    print(f"   - è®­ç»ƒæ•°æ®æ•°é‡: {train_data_count}")
    print(f"   - æ•°æ®æºç±»å‹: {data_source}")
    print(f"   - è¦é¢„æµ‹çš„æ­¥éª¤: {step_names_to_predict}")
    
    if data_source == "json":
        print(f"   - æ•°æ®æ–‡ä»¶: {json_file}")
        
    elif data_source == "pgsql":
        if pgsql_query:
            print(f"   - SQLæŸ¥è¯¢: {pgsql_query}")
        else:
            print(f"   - æ•°æ®è¡¨: {pgsql_table}")
            if pgsql_conditions:
                print(f"   - æŸ¥è¯¢æ¡ä»¶: {pgsql_conditions}")
    
    print(f"\nğŸ”§ æ•°æ®å¤„ç†æµç¨‹:")
    print(f"   1. æ ¹æ® StepName åˆ†ç±»")
    print(f"   2. æŒ‰ Timestamp æ’åº")
    print(f"   3. ä½¿ç”¨ Duration å€¼è¿›è¡Œé¢„æµ‹éªŒè¯")


# ========== å…¨æµç¨‹é¢„æµ‹éªŒè¯å‡½æ•° ==========
def run_full_painting_validation(df_predict, num_of_prediction, train_data_count=None, step_names_to_predict=None):
    """
    å…¨æµç¨‹é¢„æµ‹éªŒè¯æ¨¡å¼ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹éªŒè¯
    æŒ‰ç…§ç”¨æˆ·åŸå§‹æƒ³æ³•ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ + é¢„æµ‹é›†å‰aæ¡æ•°æ®è¡”æ¥ + é¢„æµ‹åYæ¡æ•°æ®å¯¹æ¯”
    
    å‚æ•°:
    - df_predict: é¢„æµ‹æ•°æ®
    - num_of_prediction: é¢„æµ‹æ•°é‡Y
    - train_data_count: è®­ç»ƒæ•°æ®æ•°é‡aï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è®¡ç®—ä¸ºtotal_count - num_of_prediction
    - step_names_to_predict: è¦é¢„æµ‹çš„æ­¥éª¤åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è¯†åˆ«æ¶‚èƒ¶æ­¥éª¤
    """
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # ç¡®å®šè¦é¢„æµ‹çš„æ­¥éª¤
    if step_names_to_predict is None:
        # è‡ªåŠ¨è¯†åˆ«æ‰€æœ‰æ¶‚èƒ¶æ­¥éª¤ï¼ˆå‘åå…¼å®¹ï¼‰
        all_steps = df_predict['StepName'].unique()
        steps_to_predict = [step for step in all_steps if 'æ¶‚èƒ¶' in step or 'ä¸‹è½®æ¶‚èƒ¶' in step]
        print(f"ğŸ¯ è‡ªåŠ¨è¯†åˆ«åˆ°æ¶‚èƒ¶æ­¥éª¤: {steps_to_predict}")
    else:
        # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ­¥éª¤åˆ—è¡¨
        steps_to_predict = step_names_to_predict
        print(f"ğŸ¯ ç”¨æˆ·æŒ‡å®šè¦é¢„æµ‹çš„æ­¥éª¤: {steps_to_predict}")
    
    # æ£€æŸ¥æ•°æ®ä¸­å®é™…å­˜åœ¨çš„æ­¥éª¤
    available_steps = df_predict['StepName'].unique()
    valid_steps = [step for step in steps_to_predict if step in available_steps]
    missing_steps = [step for step in steps_to_predict if step not in available_steps]
    
    if missing_steps:
        print(f"âš ï¸ ä»¥ä¸‹æ­¥éª¤åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_steps}")
    
    if not valid_steps:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ­¥éª¤è¿›è¡Œé¢„æµ‹ï¼")
        return
    
    print(f"ğŸ“Š æœ‰æ•ˆé¢„æµ‹æ­¥éª¤æ•°: {len(valid_steps)}")
    print(f"ğŸ“‹ å¯ç”¨æ­¥éª¤åˆ—è¡¨: {valid_steps}")
    
    for step_name in valid_steps:
        print(f"\n{'='*50}")
        print(f"ğŸ”§ éªŒè¯æ­¥éª¤ï¼š{step_name}")
        print(f"{'='*50}")
        
        try:
            # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦å­˜åœ¨
            model_file = f"models/model_{step_name}.keras"
            if not os.path.exists(model_file):
                print(f"âŒ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹")
                continue
            
            # è·å–é¢„æµ‹æ•°æ®ä¸­è¯¥æ­¥éª¤çš„æ€»æ•°é‡X
            df_predict_filtered = df_predict[df_predict["StepName"] == step_name].copy()
            if df_predict_filtered.empty:
                print(f"âŒ é¢„æµ‹æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°æ­¥éª¤æ•°æ®")
                continue
                
            total_count = len(df_predict_filtered)
            
            # è®¡ç®—è®­ç»ƒæ•°æ®é‡
            if train_data_count is None:
                # è‡ªåŠ¨è®¡ç®—ï¼šå‰X-Yä¸ªå€¼
                train_count = total_count - num_of_prediction
            else:
                # ç”¨æˆ·æŒ‡å®šï¼šå‰aä¸ªå€¼
                train_count = train_data_count
                
            if train_count <= 0 or train_count + num_of_prediction > total_count:
                print(f"âŒ æ•°æ®ä¸è¶³")
                continue
                
            # ä½¿ç”¨é¢„æµ‹æ•°æ®çš„å‰X-Yä¸ªå€¼æ„å»ºscalerï¼ˆç”¨äºæ•°æ®é¢„å¤„ç†ï¼‰
            df_train_combined = df_predict_filtered.head(train_count)
            _, _, scaler, _ = prepare_dataset(df_train_combined, step_name=step_name)
            
            # æ£€æŸ¥å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            if not os.path.exists(model_file):
                print(f"âŒ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ {model_file}")
                continue
            
            try:
                # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶é¢„æµ‹
                model = tf.keras.models.load_model(model_file, compile=False)
                pred_values, df2_filtered = predict_future_with_model(model, df_predict, scaler, step_name=step_name, predict_count=num_of_prediction)
                
            except Exception as e:
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                continue

            # æ£€æŸ¥é¢„æµ‹ç»“æœ
            if len(pred_values) > 0:
                # è·å–æŒ‡å®šèŒƒå›´çš„çœŸå®å€¼ç”¨äºå¯¹æ¯”
                start_idx = train_count
                end_idx = train_count + num_of_prediction
                true_values = df_predict_filtered["Duration"].iloc[start_idx:end_idx].values
                
                # ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
                plot_predictions_with_validation(df2_filtered, pred_values, true_values, step_name)

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ï¼š{e}")


# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    # ========== é…ç½®å‚æ•° ==========
    num_of_prediction = 5  # é¢„æµ‹æ•°é‡Yï¼Œç”¨äºå¯¹æ¯”éªŒè¯
    train_data_count = None  # è®­ç»ƒæ•°æ®æ•°é‡aï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼ŒæŒ‡å®šæ•°å­—è¡¨ç¤ºä½¿ç”¨å‰aä¸ªå€¼
    
    # ğŸ‘‰ æ•°æ®æºé…ç½®
    data_source = "pgsql"  # æ•°æ®æºç±»å‹: "json" æˆ– "pgsql"
    
    # ğŸ‘‰ è¦é¢„æµ‹çš„ StepName é…ç½®ï¼ˆæ‚¨å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹è¿™ä¸ªåˆ—è¡¨ï¼‰
    step_names_to_predict = [
        "æ‹§ç´§1",    # Tighten 1
        "æ‹§ç´§2",    # Tighten 2  
        "æ‹§ç´§3",    # Tighten 3
        "å¤¹ç´§",     # Clamp
        "æ¾å¼€",     # Loosen
        "å–é’‰1",    # Remove Nail 1
        "å–é’‰2"     # Remove Nail 2
    ]
    
    # JSON æ•°æ®æºé…ç½®
    predict_data_file = "data/OnlyPainting_Paint_Data1_Prediction.json"  # é¢„æµ‹æ•°æ®æ–‡ä»¶
    
    # PostgreSQL æ•°æ®æºé…ç½®ï¼ˆå½“ data_source = "pgsql" æ—¶ä½¿ç”¨ï¼‰
    pgsql_query = None  # è‡ªå®šä¹‰SQLæŸ¥è¯¢è¯­å¥ï¼Œä¾‹å¦‚: "SELECT StepName, Duration, Timestamp FROM Beats_of_M8_liangainingjin WHERE StepName LIKE '%æ¶‚èƒ¶%'"
    pgsql_table = '"Beats_of_M8_liangainingjin"'  # è¡¨å
    
    # ========== æ•°æ®åŠ è½½å’Œé…ç½® ==========
    # ç”Ÿæˆ PostgreSQL WHERE æ¡ä»¶
    pgsql_conditions = build_pgsql_conditions(step_names_to_predict)
    
    # åŠ è½½é¢„æµ‹æ•°æ®
    df_predict = load_prediction_data(
        data_source=data_source,
        json_file=predict_data_file,
        pgsql_query=pgsql_query,
        pgsql_table=pgsql_table,
        pgsql_conditions=pgsql_conditions
    )
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print_prediction_config(
        num_of_prediction=num_of_prediction,
        train_data_count=train_data_count,
        data_source=data_source,
        step_names_to_predict=step_names_to_predict,
        json_file=predict_data_file,
        pgsql_query=pgsql_query,
        pgsql_table=pgsql_table,
        pgsql_conditions=pgsql_conditions
    )
    
    run_full_painting_validation(df_predict, num_of_prediction, train_data_count, step_names_to_predict)
    print(f"\nğŸ‰ å…¨æµç¨‹é¢„æµ‹éªŒè¯å®Œæˆï¼")

