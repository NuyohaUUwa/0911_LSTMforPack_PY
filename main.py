import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam  # æ­£ç¡®è·¯å¾„
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.preprocessing import MinMaxScaler
import os
import tensorflow as tf
from datetime import datetime
from typing import Optional, Dict, Any

np.random.seed(250)
tf.random.set_seed(250)


# ========== 1. æ•°æ®è¯»å– ==========
def load_json_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return pd.DataFrame(data["Items"])


# ========== 2. é¢„å¤„ç†å‡½æ•° ==========
def prepare_dataset(df, step_name="zq", seq_len=10):
    df_filtered = df[df["StepName"] == step_name].copy()
    if df_filtered.empty:
        raise ValueError(f"âŒ æ­¥éª¤ {step_name} æ²¡æœ‰æ•°æ®ï¼")

    print(f"\nğŸ“Š Step = '{step_name}' åŸå§‹ Duration æ•°æ®:", df_filtered["Duration"].head(10).tolist())
    print("ğŸ“ NaN æ•°é‡ï¼š", df_filtered["Duration"].isna().sum())

    df_filtered = df_filtered.dropna(subset=["Duration"])
    df_filtered["Duration"] = df_filtered["Duration"].astype(float)

    durations = df_filtered["Duration"].values.reshape(-1, 1)
    durations = durations[~np.isnan(durations)]
    durations = durations[~np.isinf(durations)]

    if len(durations) <= seq_len:
        raise ValueError(f"âŒ æ¸…æ´—åæ•°æ®é‡ä¸è¶³ï¼ˆ{len(durations)} æ¡ï¼‰ï¼Œæ— æ³•ç”Ÿæˆè®­ç»ƒåºåˆ—")

    scaler = MinMaxScaler()
    durations_scaled = scaler.fit_transform(durations.reshape(-1, 1))

    X, y = [], []
    for i in range(len(durations_scaled) - seq_len):
        X.append(durations_scaled[i:i + seq_len])
        y.append(durations_scaled[i + seq_len])

    print("ğŸ§ª ç¤ºä¾‹å½’ä¸€åŒ– X[0]:", X[0].flatten())
    print("ğŸ§ª ç¤ºä¾‹å½’ä¸€åŒ– y[0]:", y[0])
    print("ğŸ“ˆ Scaler min_:", scaler.data_min_)
    print("ğŸ“ˆ Scaler range_:", scaler.data_range_)

    return np.array(X), np.array(y), scaler, df_filtered


# ========== 3. æ¨¡å‹æ„å»º ==========
def build_model(input_shape):
    model = Sequential([
        LSTM(16, activation='tanh', input_shape=input_shape, dropout=0.2, recurrent_dropout=0.2),
        Dense(1)
    ])
    model.compile(optimizer=Adam(learning_rate=0.0005, clipvalue=1.0), loss='mse')
    return model


# ========== 4. æ— é™å¾ªç¯è®­ç»ƒ ==========
def infinite_training(X, y, model_path="model_checkpoint.keras", max_epochs=None, step_name="æœªçŸ¥æ­¥éª¤", training_type="full"):
    """
    æ”¯æŒæ— é™è®­ç»ƒæˆ–å›ºå®šè½®æ•°è®­ç»ƒï¼Œè‡ªåŠ¨ä¿å­˜æ¨¡å‹ï¼Œå¹¶ç»˜åˆ¶ loss æ›²çº¿ã€‚
    
    å‚æ•°ï¼š
    - max_epochs=Noneï¼šæ— é™è®­ç»ƒç›´åˆ°æ‰‹åŠ¨ä¸­æ–­
    - max_epochs=intï¼šå›ºå®šè½®æ•°è®­ç»ƒ
    """
    model = build_model((X.shape[1], X.shape[2]))

    # æ ¹æ®è®­ç»ƒç±»å‹å’Œæ•°æ®é‡è®¾ç½®ä¸åŒçš„æ—©åœç­–ç•¥
    data_size = len(X)
    if training_type == "validation":
        # éªŒè¯æ¨¡å¼ï¼šæ›´æ¿€è¿›çš„æ—©åœ
        patience = min(20, max(10, data_size // 2))
        min_delta = 0.001
    else:
        # å®Œæ•´è®­ç»ƒæ¨¡å¼ï¼šæ›´å®½æ¾çš„æ—©åœ
        patience = min(50, max(20, data_size))
        min_delta = 0.0005
    
    early_stop = EarlyStopping(
        monitor='loss', 
        patience=patience,
        min_delta=min_delta,
        restore_best_weights=True,
        verbose=1
    )
    
    print(f"ğŸ”§ æ—©åœè®¾ç½®: patience={patience}, min_delta={min_delta}")

    losses = []
    epoch = 0

    if max_epochs is None:
        # æ— é™è®­ç»ƒï¼ˆå»ºè®®Ctrl+Cæ‰“æ–­ï¼‰
        try:
            while True:
                history = model.fit(X, y, epochs=1, verbose=0, callbacks=[early_stop])
                epoch += 1
                loss = history.history['loss'][0]
                losses.append(loss)

                if epoch % 10 == 0:
                    print(f"ğŸ“š ç¬¬ {epoch} æ¬¡è®­ç»ƒï¼ŒæŸå¤±: {loss:.6f}")
                    model.save(model_path)
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ æ‰‹åŠ¨ç»ˆæ­¢è®­ç»ƒï¼ˆç¬¬ {epoch} æ¬¡ï¼‰")
    else:
        # å›ºå®šæ¬¡æ•°è®­ç»ƒ
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆå…± {max_epochs} æ¬¡ï¼‰...")
        history = model.fit(X, y, epochs=max_epochs, callbacks=[early_stop], verbose=1)
        losses = history.history['loss']
        print("âœ… å›ºå®šè½®è®­ç»ƒå®Œæˆ")

    # æœ€åä¿å­˜ä¸€æ¬¡æ¨¡å‹
    # ä¿å­˜æ¨¡å‹ - ä½¿ç”¨æ–°çš„Kerasæ ¼å¼
    model.save(model_path)
    print(f"ğŸ“¦ æ¨¡å‹å·²ä¿å­˜è‡³ {model_path}")
    
    final_loss = model.evaluate(X, y, verbose=0)
    print(f"ğŸ“‰ æœ€ç»ˆæŸå¤±: {final_loss:.6f}")

    # ========== ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿ ========== #
    plt.figure(figsize=(10, 4))
    plt.plot(losses, label="Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f"{step_name} è®­ç»ƒè¿‡ç¨‹ Loss æ›²çº¿")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    
    # ä¿å­˜è®­ç»ƒæŸå¤±æ›²çº¿
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    filename = f"All_plotpics/TrainingLoss_{step_name}_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜: {filename}")
    
    plt.show()
    
    return model


# ========== 5. é¢„æµ‹å‡½æ•° ==========
def predict_future(df, model_path, scaler, step_name="zq", seq_len=10, predict_count=10):
    try:
        model = load_model(model_path)
    except Exception as e:
        raise ValueError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    
    # è·å–ç›®æ ‡æ­¥éª¤çš„æ•°æ®
    df_filtered = df[df["StepName"] == step_name].copy()
    
    # æ¸…ç† NaN å€¼å¹¶ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
    if df_filtered.empty:
        raise ValueError(f"âŒ '{step_name}' æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å½•")
        
    # ç§»é™¤ Duration ä¸º NaN çš„è®°å½•
    df_filtered = df_filtered.dropna(subset=["Duration"])
    
    # é‡ç½®ç´¢å¼•ä»¥ä¾¿åç»­ä½¿ç”¨è´Ÿç´¢å¼• [-seq_len:]
    df_filtered.reset_index(drop=True, inplace=True)
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹
    if len(df_filtered) < seq_len:
        raise ValueError(f"âŒ '{step_name}' çš„æœ‰æ•ˆæ•°æ®ä¸è¶³ {seq_len} æ¡ï¼Œä»…æœ‰ {len(df_filtered)} æ¡")
    
    # è½¬æ¢æ•°æ®ç±»å‹å¹¶æ£€æŸ¥å¼‚å¸¸å€¼
    durations = df_filtered["Duration"].astype(float).values.reshape(-1, 1)
    
    # æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥
    if np.any(np.isnan(durations)):
        # è®°å½•æœ‰é—®é¢˜çš„è¡Œç´¢å¼•
        nan_indices = np.where(np.isnan(durations.flatten()))[0]
        problem_timestamps = df_filtered.iloc[nan_indices]["Timestamp"].tolist()
        raise ValueError(
            f"âŒ '{step_name}' æ•°æ®å¼‚å¸¸: {len(nan_indices)} æ¡è®°å½• Duration å€¼æ— æ•ˆ\n"
            f"é—®é¢˜è®°å½•æ—¶é—´: {problem_timestamps}"
        )
    
    # ç¼©æ”¾æ•°æ®
    durations_scaled = scaler.transform(durations)
    
    # è·å–æœ€æ–°çš„åºåˆ—ä½œä¸ºæ¨¡å‹è¾“å…¥
    current_seq = durations_scaled[-seq_len:]
    
    # åˆ›å»ºé¢„æµ‹å­˜å‚¨åˆ—è¡¨
    predictions = []
    
    # æ·»åŠ è¯Šæ–­æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
    print(f"âœ… æˆåŠŸè·å– {len(df_filtered)} æ¡ '{step_name}' æœ‰æ•ˆè®°å½•")
    print(f"æœ€æ–°{seq_len}æ¡åºåˆ—æ—¶é—´èŒƒå›´: {df_filtered['Timestamp'].iloc[-seq_len]} - {df_filtered['Timestamp'].iloc[-1]}")

    for i in range(predict_count):
        input_seq = current_seq.reshape(1, seq_len, 1)
        pred = model.predict(input_seq, verbose=0)[0][0]
        print(f"ğŸ” åŸå§‹é¢„æµ‹å€¼ï¼ˆå½’ä¸€åŒ–ï¼‰: {pred}")
        if np.isnan(pred) or np.isinf(pred):
            print(f"ğŸš« é¢„æµ‹å¤±è´¥: âŒ ç¬¬ {i + 1} æ¬¡é¢„æµ‹ç»“æœä¸º NaN/infï¼Œè¯·æ£€æŸ¥æ¨¡å‹æˆ–è®­ç»ƒæ•°æ®")
            break
        predictions.append(pred)
        current_seq = np.append(current_seq[1:], [[pred]], axis=0)

    if len(predictions) == 0:
        print("âš ï¸ æ— æœ‰æ•ˆé¢„æµ‹ç»“æœï¼Œç»ˆæ­¢åç»­å¤„ç†ã€‚")
        return [], df_filtered

    predictions_array = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
    print("ğŸ”® é¢„æµ‹å€¼åˆ—è¡¨ï¼ˆåå½’ä¸€åŒ–åï¼‰:")
    for i, val in enumerate(predictions_array, 1):
        print(f"é¢„æµ‹{i}: {val:.4f} ç§’")

    return predictions_array, df_filtered


def predict_future_with_model(model, df, scaler, step_name="zq", seq_len=10, predict_count=10):
    """ä½¿ç”¨æ¨¡å‹å¯¹è±¡è¿›è¡Œé¢„æµ‹ï¼Œé¿å…æ–‡ä»¶åŠ è½½é—®é¢˜"""
    # è·å–ç›®æ ‡æ­¥éª¤çš„æ•°æ®
    df_filtered = df[df["StepName"] == step_name].copy()
    
    # æ¸…ç† NaN å€¼å¹¶ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
    if df_filtered.empty:
        raise ValueError(f"âŒ '{step_name}' æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å½•")
        
    # ç§»é™¤ Duration ä¸º NaN çš„è®°å½•
    df_filtered = df_filtered.dropna(subset=["Duration"])
    
    # é‡ç½®ç´¢å¼•ä»¥ä¾¿åç»­ä½¿ç”¨è´Ÿç´¢å¼• [-seq_len:]
    df_filtered.reset_index(drop=True, inplace=True)
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹
    if len(df_filtered) < seq_len:
        raise ValueError(f"âŒ '{step_name}' çš„æœ‰æ•ˆæ•°æ®ä¸è¶³ {seq_len} æ¡ï¼Œä»…æœ‰ {len(df_filtered)} æ¡")
    
    # è½¬æ¢æ•°æ®ç±»å‹å¹¶æ£€æŸ¥å¼‚å¸¸å€¼
    durations = df_filtered["Duration"].astype(float).values.reshape(-1, 1)
    
    # æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥
    if np.any(np.isnan(durations)):
        # è®°å½•æœ‰é—®é¢˜çš„è¡Œç´¢å¼•
        nan_indices = np.where(np.isnan(durations.flatten()))[0]
        problem_timestamps = df_filtered.iloc[nan_indices]["Timestamp"].tolist()
        raise ValueError(
            f"âŒ '{step_name}' æ•°æ®å¼‚å¸¸: {len(nan_indices)} æ¡è®°å½• Duration å€¼æ— æ•ˆ\n"
            f"é—®é¢˜è®°å½•æ—¶é—´: {problem_timestamps}"
        )
    
    # ç¼©æ”¾æ•°æ®
    durations_scaled = scaler.transform(durations)
    
    # è·å–æœ€æ–°çš„åºåˆ—ä½œä¸ºæ¨¡å‹è¾“å…¥
    current_seq = durations_scaled[-seq_len:]
    
    # åˆ›å»ºé¢„æµ‹å­˜å‚¨åˆ—è¡¨
    predictions = []
    
    # æ·»åŠ è¯Šæ–­æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
    print(f"âœ… æˆåŠŸè·å– {len(df_filtered)} æ¡ '{step_name}' æœ‰æ•ˆè®°å½•")
    print(f"æœ€æ–°{seq_len}æ¡åºåˆ—æ—¶é—´èŒƒå›´: {df_filtered['Timestamp'].iloc[-seq_len]} - {df_filtered['Timestamp'].iloc[-1]}")

    for i in range(predict_count):
        input_seq = current_seq.reshape(1, seq_len, 1)
        pred = model.predict(input_seq, verbose=0)[0][0]
        print(f"ğŸ” åŸå§‹é¢„æµ‹å€¼ï¼ˆå½’ä¸€åŒ–ï¼‰: {pred}")
        if np.isnan(pred) or np.isinf(pred):
            print(f"ğŸš« é¢„æµ‹å¤±è´¥: âŒ ç¬¬ {i + 1} æ¬¡é¢„æµ‹ç»“æœä¸º NaN/infï¼Œè¯·æ£€æŸ¥æ¨¡å‹æˆ–è®­ç»ƒæ•°æ®")
            break
        predictions.append(pred)
        current_seq = np.append(current_seq[1:], [[pred]], axis=0)

    if len(predictions) == 0:
        print("âš ï¸ æ— æœ‰æ•ˆé¢„æµ‹ç»“æœï¼Œç»ˆæ­¢åç»­å¤„ç†ã€‚")
        return [], df_filtered

    predictions_array = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
    print("ğŸ”® é¢„æµ‹å€¼åˆ—è¡¨ï¼ˆåå½’ä¸€åŒ–åï¼‰:")
    for i, val in enumerate(predictions_array, 1):
        print(f"é¢„æµ‹{i}: {val:.4f} ç§’")

    return predictions_array, df_filtered


# ========== 6. å¯è§†åŒ– ==========
def plot_predictions(df_filtered, pred_durations):
    plt.figure(figsize=(16, 6))
    plt.plot(df_filtered["Duration"].values, label="å†å²æ•°æ®")
    plt.plot(range(len(df_filtered), len(df_filtered) + len(pred_durations)), pred_durations, "o-", color="orange",
             label="é¢„æµ‹")

    for i, v in enumerate(df_filtered["Duration"].values):
        plt.text(i, v + 0.03, f"{v:.2f}", ha="center", fontsize=8)
    for i, v in enumerate(pred_durations):
        plt.text(len(df_filtered) + i, v + 0.03, f"{v:.2f}", ha="center", fontsize=8, color="orange")

    labels = df_filtered["StepName"].tolist() + [f"é¢„æµ‹{i + 1}" for i in range(len(pred_durations))]
    plt.xticks(ticks=range(len(labels)), labels=labels, rotation=45)
    plt.xlabel("æ­¥éª¤")
    plt.ylabel("Durationï¼ˆç§’ï¼‰")
    plt.title(f"{df_filtered.iloc[0]['StepName']} æ­¥éª¤è€—æ—¶é¢„æµ‹")
    plt.legend()
    plt.tight_layout()
    
    # ä¿å­˜é¢„æµ‹ç»“æœå›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    step_name = df_filtered.iloc[0]['StepName']
    filename = f"All_plotpics/Prediction_{step_name}_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š é¢„æµ‹ç»“æœå›¾è¡¨å·²ä¿å­˜: {filename}")
    
    plt.show()


def plot_predictions_with_validation(df_filtered, pred_durations, true_values, step_name):
    """ç»˜åˆ¶åŒ…å«éªŒè¯å¯¹æ¯”çš„é¢„æµ‹å›¾è¡¨"""
    plt.figure(figsize=(18, 8))
    
    # è®­ç»ƒæ•°æ®ï¼ˆå†å²æ•°æ®ï¼‰
    train_data = df_filtered["Duration"].values
    plt.plot(train_data, "b-", label="è®­ç»ƒæ•°æ®", linewidth=2, marker='o', markersize=4)
    
    # é¢„æµ‹å€¼
    pred_start_idx = len(train_data)
    pred_indices = range(pred_start_idx, pred_start_idx + len(pred_durations))
    plt.plot(pred_indices, pred_durations, "r-", label="é¢„æµ‹å€¼", linewidth=2, marker='s', markersize=6)
    
    # çœŸå®å€¼ï¼ˆç”¨äºéªŒè¯ï¼‰
    true_indices = range(pred_start_idx, pred_start_idx + len(true_values))
    plt.plot(true_indices, true_values, "g-", label="çœŸå®å€¼", linewidth=2, marker='^', markersize=6)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(train_data):
        plt.text(i, v + 0.05, f"{v:.2f}", ha="center", fontsize=8, color="blue")
    
    for i, v in enumerate(pred_durations):
        plt.text(pred_start_idx + i, v + 0.05, f"{v:.2f}", ha="center", fontsize=8, color="red")
    
    for i, v in enumerate(true_values):
        plt.text(pred_start_idx + i, v - 0.1, f"{v:.2f}", ha="center", fontsize=8, color="green")
    
    # æ·»åŠ è¯¯å·®çº¿
    for i, (pred, true) in enumerate(zip(pred_durations, true_values)):
        plt.plot([pred_start_idx + i, pred_start_idx + i], [pred, true], 
                "k--", alpha=0.5, linewidth=1)
        error = abs(pred - true)
        plt.text(pred_start_idx + i, (pred + true) / 2, f"è¯¯å·®:{error:.3f}", 
                ha="center", fontsize=7, color="purple")
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    plt.xlabel("æ•°æ®ç‚¹ç´¢å¼•")
    plt.ylabel("Durationï¼ˆç§’ï¼‰")
    plt.title(f"{step_name} æ­¥éª¤è€—æ—¶é¢„æµ‹éªŒè¯å¯¹æ¯”\nè®­ç»ƒæ•°æ®: {len(train_data)}ä¸ªç‚¹, é¢„æµ‹: {len(pred_durations)}ä¸ªç‚¹")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ åˆ†ç•Œçº¿
    plt.axvline(x=pred_start_idx-0.5, color='gray', linestyle=':', alpha=0.7, label="è®­ç»ƒ/é¢„æµ‹åˆ†ç•Œçº¿")
    
    plt.tight_layout()
    
    # ä¿å­˜éªŒè¯å¯¹æ¯”å›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    filename = f"All_plotpics/Validation_{step_name}_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š éªŒè¯å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {filename}")
    
    plt.show()
    
    # è®¡ç®—å¹¶æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
    mae = np.mean(np.abs(pred_durations - true_values))
    mse = np.mean((pred_durations - true_values) ** 2)
    rmse = np.sqrt(mse)
    
    print(f"\nğŸ“Š é¢„æµ‹æ€§èƒ½è¯„ä¼°:")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.4f} ç§’")
    print(f"   å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
    print(f"   å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.4f} ç§’")


# ========== 7. å­—ä½“é…ç½® ==========


# ========== 8. å…¨æµç¨‹è®­ç»ƒå‡½æ•° ==========
def run_full_painting_workflow(df_train, train_epochs):
    """
    å…¨æµç¨‹è®­ç»ƒæ¨¡å¼ï¼šè‡ªåŠ¨è¯†åˆ«å¹¶è®­ç»ƒæ‰€æœ‰æ¶‚èƒ¶æ­¥éª¤
    
    å‚æ•°:
    - df_train: è®­ç»ƒæ•°æ®
    - train_epochs: è®­ç»ƒè½®æ¬¡
    """
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # è‡ªåŠ¨è¯†åˆ«æ‰€æœ‰æ¶‚èƒ¶æ­¥éª¤
    all_steps = df_train['StepName'].unique()
    painting_steps = [step for step in all_steps if 'æ¶‚èƒ¶' in step or 'ä¸‹è½®æ¶‚èƒ¶' in step]
    
    print(f"ğŸ¯ è‡ªåŠ¨è¯†åˆ«åˆ°æ¶‚èƒ¶æ­¥éª¤: {painting_steps}")
    print(f"ğŸ“Š æ€»æ­¥éª¤æ•°: {len(painting_steps)}")
    
    for step_name in painting_steps:
        print(f"\n\n================= ğŸš€ å…¨æµç¨‹è®­ç»ƒï¼š{step_name} =================")
        
        try:
            # ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒ
            X_train, y_train, scaler, df_filtered = prepare_dataset(df_train, step_name=step_name)
            
            print(f"ğŸ“Š è®­ç»ƒæ•°æ®ä¸­ {step_name} æ­¥éª¤æ€»æ•°é‡: {len(df_filtered)}")
            print(f"ğŸ“š ä½¿ç”¨å…¨éƒ¨ {len(df_filtered)} ä¸ªæ•°æ®ç‚¹è¿›è¡Œå®Œæ•´è®­ç»ƒ...")
            
            model_file = f"model_{step_name}.keras"
            
            # å®Œæ•´è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
            print(f"ğŸš€ å¼€å§‹å®Œæ•´è®­ç»ƒï¼ˆå…± {train_epochs} è½®ï¼‰...")
            model = infinite_training(X_train, y_train, model_path=model_file, max_epochs=train_epochs, step_name=step_name, training_type="full")
            
            print(f"âœ… {step_name} é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜è‡³ {model_file}")
            print(f"ğŸ“ˆ è®­ç»ƒæ•°æ®é‡: {len(df_filtered)} æ¡")
            print(f"ğŸ¯ æ¨¡å‹å¯ç”¨äºåç»­é¢„æµ‹ä»»åŠ¡")

        except Exception as e:
            print(f"âŒ æ­¥éª¤ {step_name} è®­ç»ƒå¤±è´¥ï¼š{e}")


def run_full_painting_validation(df_predict, num_of_prediction, train_data_count=None):
    """
    å…¨æµç¨‹é¢„æµ‹éªŒè¯æ¨¡å¼ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹éªŒè¯
    æŒ‰ç…§ç”¨æˆ·åŸå§‹æƒ³æ³•ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ + é¢„æµ‹é›†å‰aæ¡æ•°æ®è¡”æ¥ + é¢„æµ‹åYæ¡æ•°æ®å¯¹æ¯”
    
    å‚æ•°:
    - df_predict: é¢„æµ‹æ•°æ®
    - num_of_prediction: é¢„æµ‹æ•°é‡Y
    - train_data_count: è®­ç»ƒæ•°æ®æ•°é‡aï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è®¡ç®—ä¸ºtotal_count - num_of_prediction
    """
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # è‡ªåŠ¨è¯†åˆ«æ‰€æœ‰æ¶‚èƒ¶æ­¥éª¤
    all_steps = df_predict['StepName'].unique()
    painting_steps = [step for step in all_steps if 'æ¶‚èƒ¶' in step or 'ä¸‹è½®æ¶‚èƒ¶' in step]
    
    print(f"ğŸ¯ è‡ªåŠ¨è¯†åˆ«åˆ°æ¶‚èƒ¶æ­¥éª¤: {painting_steps}")
    print(f"ğŸ“Š æ€»æ­¥éª¤æ•°: {len(painting_steps)}")
    
    for step_name in painting_steps:
        print(f"\n\n================= ğŸ”§ å…¨æµç¨‹éªŒè¯ï¼š{step_name} =================")
        
        try:
            # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦å­˜åœ¨
            model_file = f"model_{step_name}.keras"
            if not os.path.exists(model_file):
                print(f"âŒ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ {model_file}ï¼Œè¯·å…ˆè¿è¡Œå®Œæ•´è®­ç»ƒæ¨¡å¼ï¼")
                continue
            
            # è·å–é¢„æµ‹æ•°æ®ä¸­è¯¥æ­¥éª¤çš„æ€»æ•°é‡X
            df_predict_filtered = df_predict[df_predict["StepName"] == step_name].copy()
            if df_predict_filtered.empty:
                print(f"âŒ é¢„æµ‹æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°æ­¥éª¤ {step_name} çš„æ•°æ®ï¼")
                continue
                
            total_count = len(df_predict_filtered)
            print(f"ğŸ“Š é¢„æµ‹æ•°æ®ä¸­ {step_name} æ­¥éª¤æ€»æ•°é‡: {total_count}")
            
            # è®¡ç®—è®­ç»ƒæ•°æ®é‡
            if train_data_count is None:
                # è‡ªåŠ¨è®¡ç®—ï¼šå‰X-Yä¸ªå€¼
                train_count = total_count - num_of_prediction
            else:
                # ç”¨æˆ·æŒ‡å®šï¼šå‰aä¸ªå€¼
                train_count = train_data_count
                
            if train_count <= 0:
                print(f"âŒ è®­ç»ƒæ•°æ®ä¸è¶³ï¼æŒ‡å®šæ•°é‡ {train_count} æ— æ•ˆ")
                continue
                
            if train_count + num_of_prediction > total_count:
                print(f"âŒ æ•°æ®ä¸è¶³ï¼éœ€è¦ {train_count + num_of_prediction} æ¡ï¼Œä½†åªæœ‰ {total_count} æ¡")
                continue
                
            print(f"ğŸ“š ä½¿ç”¨å‰ {train_count} ä¸ªå€¼ä½œä¸ºè¾“å…¥åºåˆ—ï¼Œé¢„æµ‹ç¬¬ {train_count+1} åˆ°ç¬¬ {train_count+num_of_prediction} ä¸ªå€¼")
            
            # ä½¿ç”¨é¢„æµ‹æ•°æ®çš„å‰X-Yä¸ªå€¼æ„å»ºscalerï¼ˆç”¨äºæ•°æ®é¢„å¤„ç†ï¼‰
            df_train_combined = df_predict_filtered.head(train_count)
            _, _, scaler, _ = prepare_dataset(df_train_combined, step_name=step_name)
            
            # æ£€æŸ¥å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            if not os.path.exists(model_file):
                print(f"âŒ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ {model_file}")
                print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œå®Œæ•´è®­ç»ƒæ¨¡å¼ç”Ÿæˆé¢„è®­ç»ƒæ¨¡å‹")
                continue
            
            try:
                # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
                print(f"ğŸ”„ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_file}")
                model = tf.keras.models.load_model(model_file, compile=False)
                print(f"âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
                
                # ä½¿ç”¨é¢„æµ‹é›†å‰aæ¡æ•°æ®è¡”æ¥é¢„è®­ç»ƒæ¨¡å‹
                print(f"ğŸ”— ä½¿ç”¨å‰ {train_count} æ¡æ•°æ®è¡”æ¥é¢„è®­ç»ƒæ¨¡å‹...")
                
                # ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œä¸éœ€è¦é‡æ–°è®­ç»ƒ
                pred_values, df2_filtered = predict_future_with_model(model, df_predict, scaler, step_name=step_name, predict_count=num_of_prediction)
                
            except Exception as e:
                print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print(f"ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ {model_file} æ˜¯å¦å®Œæ•´")
                continue

            # æ£€æŸ¥é¢„æµ‹ç»“æœ
            if len(pred_values) > 0:
                # è·å–æŒ‡å®šèŒƒå›´çš„çœŸå®å€¼ç”¨äºå¯¹æ¯”
                start_idx = train_count
                end_idx = train_count + num_of_prediction
                true_values = df_predict_filtered["Duration"].iloc[start_idx:end_idx].values
                
                print(f"\nğŸ” é¢„æµ‹å€¼ vs çœŸå®å€¼å¯¹æ¯”:")
                print(f"ğŸ“Š é¢„æµ‹èŒƒå›´: ç¬¬{start_idx+1}åˆ°ç¬¬{end_idx}ä¸ªæ•°æ®ç‚¹")
                for i in range(len(pred_values)):
                    actual_idx = start_idx + i + 1
                    print(f"ç¬¬{actual_idx}ä¸ª: é¢„æµ‹={pred_values[i]:.4f}s, çœŸå®={true_values[i]:.4f}s, è¯¯å·®={abs(pred_values[i]-true_values[i]):.4f}s")
                
                # ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
                plot_predictions_with_validation(df2_filtered, pred_values, true_values, step_name)

        except Exception as e:
            print(f"âŒ æ­¥éª¤ {step_name} å¤„ç†å¤±è´¥ï¼š{e}")


# ========== 9. ä¸»æ§åˆ¶å™¨ ==========
if __name__ == "__main__":
    # ========== é…ç½®å‚æ•° ==========
    # ğŸ‘‰ è¿è¡Œæ¨¡å¼è®¾ç½®
    run_mode = "validation"  # "full_training"=å®Œæ•´è®­ç»ƒæ¨¡å¼, "validation"=é¢„æµ‹éªŒè¯æ¨¡å¼
    train_epochs = 50  # è®­ç»ƒè½®æ¬¡ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
    
    # ğŸ‘‰ é¢„æµ‹éªŒè¯è®¾ç½®ï¼ˆä»…åœ¨validationæ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
    num_of_prediction = 5  # é¢„æµ‹æ•°é‡Yï¼Œç”¨äºå¯¹æ¯”éªŒè¯
    train_data_count = 15  # è®­ç»ƒæ•°æ®æ•°é‡aï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼ŒæŒ‡å®šæ•°å­—è¡¨ç¤ºä½¿ç”¨å‰aä¸ªå€¼
    
    # ========== æ•°æ®åŠ è½½ ==========
    print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®...")
    df_train = load_json_data("OnlyPainting_Paint_Data3_Train.json")
    df_predict = load_json_data("OnlyPainting_Paint_Data1_Prediction.json")
    print("âœ… æ•°æ®åŠ è½½å®Œæˆ")
    
    # ========== æ ¹æ®æ¨¡å¼è¿è¡Œ ==========
    if run_mode == "full_training":
        print(f"\nğŸš€ å¼€å§‹å…¨æµç¨‹è®­ç»ƒæ¨¡å¼...")
        print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"   - è¿è¡Œæ¨¡å¼: å…¨æµç¨‹è®­ç»ƒ")
        print(f"   - è®­ç»ƒè½®æ¬¡: {train_epochs}")
        print(f"   - æ•°æ®æº: OnlyPainting_Paint_Data3_Train.json")
        
        run_full_painting_workflow(df_train, train_epochs)
        print(f"\nğŸ‰ å…¨æµç¨‹è®­ç»ƒå®Œæˆï¼æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜ã€‚")
        
    elif run_mode == "validation":
        print(f"\nğŸ”§ å¼€å§‹å…¨æµç¨‹é¢„æµ‹éªŒè¯æ¨¡å¼...")
        print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"   - è¿è¡Œæ¨¡å¼: å…¨æµç¨‹é¢„æµ‹éªŒè¯")
        print(f"   - é¢„æµ‹æ•°é‡: {num_of_prediction}")
        print(f"   - æ•°æ®æº: OnlyPainting_Paint_Data1_Prediction.json")
        
        run_full_painting_validation(df_predict, num_of_prediction, train_data_count)
        print(f"\nğŸ‰ å…¨æµç¨‹é¢„æµ‹éªŒè¯å®Œæˆï¼")
        
    else:
        print(f"âŒ æ— æ•ˆçš„è¿è¡Œæ¨¡å¼: {run_mode}")
        print(f"   è¯·é€‰æ‹©: 'full_training' æˆ– 'validation'")
